{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ONNX模型导出为KModel格式"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 ONNX模型直接转换为KModel格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1940903/2480207409.py:19: DeprecationWarning: `mapping.TENSOR_TYPE_TO_NP_TYPE` is now deprecated and will be removed in a future release.To silence this warning, please use `helper.tensor_dtype_to_np_dtype` instead.\n",
      "  input_dict['dtype'] = onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[onnx_type.elem_type]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Import graph...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Not supported ONNX opcode: NonZero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m# import\u001b[39;00m\n\u001b[1;32m     62\u001b[0m model_content \u001b[39m=\u001b[39m read_model_file(model_file)\n\u001b[0;32m---> 63\u001b[0m compiler\u001b[39m.\u001b[39;49mimport_onnx(model_content, import_options)\n\u001b[1;32m     65\u001b[0m \u001b[39m# compile\u001b[39;00m\n\u001b[1;32m     66\u001b[0m compiler\u001b[39m.\u001b[39mcompile()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Not supported ONNX opcode: NonZero"
     ]
    }
   ],
   "source": [
    "import nncase\n",
    "import onnxsim\n",
    "import os\n",
    "import onnx\n",
    "\n",
    "def parse_model_input_output(model_file):\n",
    "    onnx_model = onnx.load(model_file)\n",
    "    input_all = [node.name for node in onnx_model.graph.input]\n",
    "    input_initializer = [node.name for node in onnx_model.graph.initializer]\n",
    "    input_names = list(set(input_all) - set(input_initializer))\n",
    "    input_tensors = [node for node in onnx_model.graph.input if node.name in input_names]\n",
    "\n",
    "    # input\n",
    "    inputs = []\n",
    "    for _, e in enumerate(input_tensors):\n",
    "        onnx_type = e.type.tensor_type\n",
    "        input_dict = {}\n",
    "        input_dict['name'] = e.name\n",
    "        input_dict['dtype'] = onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[onnx_type.elem_type]\n",
    "        input_dict['shape'] = [(i.dim_value if i.dim_value != 0 else d) for i, d in zip(\n",
    "            onnx_type.shape.dim, [1, 3, 224, 224])]\n",
    "        inputs.append(input_dict)\n",
    "\n",
    "    return onnx_model, inputs\n",
    "\n",
    "def onnx_simplify(model_file):\n",
    "    onnx_model, inputs = parse_model_input_output(model_file)\n",
    "    onnx_model = onnx.shape_inference.infer_shapes(onnx_model)\n",
    "    input_shapes = {}\n",
    "    for input in inputs:\n",
    "        input_shapes[input['name']] = input['shape']\n",
    "\n",
    "    onnx_model, check = onnxsim.simplify(onnx_model, overwrite_input_shapes=input_shapes)\n",
    "    assert check, \"Simplified ONNX model could not be validated\"\n",
    "\n",
    "    model_file = os.path.join(os.path.dirname(model_file), 'yolo_free_large_simplified.onnx')\n",
    "    onnx.save_model(onnx_model, model_file)\n",
    "    return model_file\n",
    "\n",
    "def read_model_file(model_file):\n",
    "    with open(model_file, 'rb') as f:\n",
    "        model_content = f.read()\n",
    "    return model_content\n",
    "\n",
    "# onnx simplify\n",
    "model_file = onnx_simplify(\"../weights/onnx/11/yolo_free_large.onnx\")\n",
    "\n",
    "# compile_options\n",
    "compile_options = nncase.CompileOptions()\n",
    "compile_options.target = 'k210'\n",
    "compile_options.dump_ir = True\n",
    "compile_options.dump_asm = True\n",
    "compile_options.dump_dir = 'tmp'\n",
    "\n",
    "# compiler\n",
    "compiler = nncase.Compiler(compile_options)\n",
    "\n",
    "# import_options\n",
    "import_options = nncase.ImportOptions()\n",
    "\n",
    "# import\n",
    "model_content = read_model_file(model_file)\n",
    "compiler.import_onnx(model_content, import_options)\n",
    "\n",
    "# compile\n",
    "compiler.compile()\n",
    "\n",
    "# kmodel\n",
    "kmodel = compiler.gencode_tobytes()\n",
    "name = os.path.basename(model_file).split(\".\")[0]\n",
    "with open(f'{name}.kmodel', 'wb') as f:\n",
    "    f.write(kmodel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 ONNX模型转换成TFlite格式再转换成KModel格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xd324/anaconda3/envs/pt20/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2023-05-07 13:51:19.196991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-07 13:51:19.197144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-07 13:51:19.217966: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-05-07 13:51:24.316359: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor '1508' with dtype float and shape [2000,4]\n",
      "\t [[{{node 1508}}]]\n",
      "2023-05-07 13:51:24.360421: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor '1478' with dtype float and shape [20,512,1,1]\n",
      "\t [[{{node 1478}}]]\n",
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "2023-05-07 13:51:36.625192: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'serving_default_images' with dtype float and shape [1,3,640,640]\n",
      "\t [[{{node serving_default_images}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../weights/tflite/yolo_free_large_simplified.pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../weights/tflite/yolo_free_large_simplified.pb/assets\n",
      "2023-05-07 13:51:49.693189: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'serving_default_images' with dtype float and shape [1,3,640,640]\n",
      "\t [[{{node serving_default_images}}]]\n",
      "2023-05-07 13:51:49.742560: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-05-07 13:51:49.742581: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-05-07 13:51:49.743044: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ../weights/tflite/yolo_free_large_simplified.pb\n",
      "2023-05-07 13:51:49.762304: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-05-07 13:51:49.762333: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ../weights/tflite/yolo_free_large_simplified.pb\n",
      "2023-05-07 13:51:49.825663: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
      "2023-05-07 13:51:49.827279: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-05-07 13:51:49.947082: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: ../weights/tflite/yolo_free_large_simplified.pb\n",
      "2023-05-07 13:51:50.023040: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 279998 microseconds.\n",
      "2023-05-07 13:51:50.222868: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "loc(callsite(callsite(fused[\"Cast:\", \"onnx_tf_prefix_/Div/Cast@__inference___call___1379\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_1524\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Cast' op is neither a custom op nor a flex op\n",
      "loc(callsite(callsite(fused[\"RealDiv:\", \"onnx_tf_prefix_/Div@__inference___call___1379\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_1524\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.RealDiv' op is neither a custom op nor a flex op\n",
      "loc(callsite(callsite(fused[\"Cast:\", \"Cast_18@__inference___call___1379\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_1524\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Cast' op is neither a custom op nor a flex op\n",
      "error: failed while converting: 'main': \n",
      "Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \n",
      "TF Select ops: Cast, RealDiv\n",
      "Details:\n",
      "\ttf.Cast(tensor<?xf64>) -> (tensor<?xi64>) : {Truncate = false, device = \"\"}\n",
      "\ttf.Cast(tensor<?xi64>) -> (tensor<?xf64>) : {Truncate = false, device = \"\"}\n",
      "\ttf.RealDiv(tensor<?xf64>, tensor<f64>) -> (tensor<?xf64>) : {device = \"\"}\n",
      "\n"
     ]
    },
    {
     "ename": "ConverterError",
     "evalue": "<unknown>:0: error: loc(callsite(callsite(fused[\"Cast:\", \"onnx_tf_prefix_/Div/Cast@__inference___call___1379\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_1524\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Cast' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Cast:\", \"onnx_tf_prefix_/Div/Cast@__inference___call___1379\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_1524\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"RealDiv:\", \"onnx_tf_prefix_/Div@__inference___call___1379\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_1524\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.RealDiv' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"RealDiv:\", \"onnx_tf_prefix_/Div@__inference___call___1379\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_1524\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Cast:\", \"Cast_18@__inference___call___1379\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_1524\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Cast' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Cast:\", \"Cast_18@__inference___call___1379\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_1524\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \nTF Select ops: Cast, RealDiv\nDetails:\n\ttf.Cast(tensor<?xf64>) -> (tensor<?xi64>) : {Truncate = false, device = \"\"}\n\ttf.Cast(tensor<?xi64>) -> (tensor<?xf64>) : {Truncate = false, device = \"\"}\n\ttf.RealDiv(tensor<?xf64>, tensor<f64>) -> (tensor<?xf64>) : {device = \"\"}\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConverterError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m## 启用量化配置\u001b[39;00m\n\u001b[1;32m     13\u001b[0m converter\u001b[39m.\u001b[39moptimizations \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mlite\u001b[39m.\u001b[39mOptimize\u001b[39m.\u001b[39mDEFAULT]\n\u001b[0;32m---> 14\u001b[0m tf_lite_model \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39;49mconvert()\n\u001b[1;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(TFLITE_PATH, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     16\u001b[0m     f\u001b[39m.\u001b[39mwrite(tf_lite_model)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt20/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:962\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(convert_func)\n\u001b[1;32m    960\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    961\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 962\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_and_export_metrics(convert_func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt20/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:940\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_conversion_params_metric()\n\u001b[1;32m    939\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mprocess_time()\n\u001b[0;32m--> 940\u001b[0m result \u001b[39m=\u001b[39m convert_func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    941\u001b[0m elapsed_time_ms \u001b[39m=\u001b[39m (time\u001b[39m.\u001b[39mprocess_time() \u001b[39m-\u001b[39m start_time) \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m    942\u001b[0m \u001b[39mif\u001b[39;00m result:\n",
      "File \u001b[0;32m~/anaconda3/envs/pt20/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1247\u001b[0m, in \u001b[0;36mTFLiteSavedModelConverterV2.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_debug_info \u001b[39m=\u001b[39m _get_debug_info(\n\u001b[1;32m   1244\u001b[0m       _convert_debug_info_func(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trackable_obj\u001b[39m.\u001b[39mgraph_debug_info),\n\u001b[1;32m   1245\u001b[0m       graph_def)\n\u001b[0;32m-> 1247\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_from_saved_model(graph_def)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt20/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1130\u001b[0m, in \u001b[0;36mTFLiteConverterBaseV2._convert_from_saved_model\u001b[0;34m(self, graph_def)\u001b[0m\n\u001b[1;32m   1127\u001b[0m converter_kwargs\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_base_converter_args())\n\u001b[1;32m   1128\u001b[0m converter_kwargs\u001b[39m.\u001b[39mupdate(quant_mode\u001b[39m.\u001b[39mconverter_flags())\n\u001b[0;32m-> 1130\u001b[0m result \u001b[39m=\u001b[39m _convert_saved_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconverter_kwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimize_tflite_model(\n\u001b[1;32m   1132\u001b[0m     result, quant_mode, quant_io\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_new_quantizer)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt20/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:212\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     report_error_message(\u001b[39mstr\u001b[39m(converter_error))\n\u001b[0;32m--> 212\u001b[0m   \u001b[39mraise\u001b[39;00m converter_error \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m  \u001b[39m# Re-throws the exception.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m    214\u001b[0m   report_error_message(\u001b[39mstr\u001b[39m(error))\n",
      "File \u001b[0;32m~/anaconda3/envs/pt20/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    204\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m   \u001b[39mexcept\u001b[39;00m ConverterError \u001b[39mas\u001b[39;00m converter_error:\n\u001b[1;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m converter_error\u001b[39m.\u001b[39merrors:\n",
      "File \u001b[0;32m~/anaconda3/envs/pt20/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:832\u001b[0m, in \u001b[0;36mconvert_saved_model\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m model_flags \u001b[39m=\u001b[39m build_model_flags(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    831\u001b[0m conversion_flags \u001b[39m=\u001b[39m build_conversion_flags(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 832\u001b[0m data \u001b[39m=\u001b[39m convert(\n\u001b[1;32m    833\u001b[0m     model_flags\u001b[39m.\u001b[39;49mSerializeToString(),\n\u001b[1;32m    834\u001b[0m     conversion_flags\u001b[39m.\u001b[39;49mSerializeToString(),\n\u001b[1;32m    835\u001b[0m     input_data_str\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    836\u001b[0m     debug_info_str\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    837\u001b[0m     enable_mlir_converter\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    838\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/pt20/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:322\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(model_flags_str, conversion_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39mfor\u001b[39;00m error_data \u001b[39min\u001b[39;00m _metrics_wrapper\u001b[39m.\u001b[39mretrieve_collected_errors():\n\u001b[1;32m    321\u001b[0m       converter_error\u001b[39m.\u001b[39mappend_error(error_data)\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mraise\u001b[39;00m converter_error\n\u001b[1;32m    324\u001b[0m \u001b[39mreturn\u001b[39;00m _run_deprecated_conversion_binary(model_flags_str,\n\u001b[1;32m    325\u001b[0m                                          conversion_flags_str, input_data_str,\n\u001b[1;32m    326\u001b[0m                                          debug_info_str)\n",
      "\u001b[0;31mConverterError\u001b[0m: <unknown>:0: error: loc(callsite(callsite(fused[\"Cast:\", \"onnx_tf_prefix_/Div/Cast@__inference___call___1379\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_1524\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Cast' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Cast:\", \"onnx_tf_prefix_/Div/Cast@__inference___call___1379\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_1524\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"RealDiv:\", \"onnx_tf_prefix_/Div@__inference___call___1379\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_1524\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.RealDiv' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"RealDiv:\", \"onnx_tf_prefix_/Div@__inference___call___1379\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_1524\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Cast:\", \"Cast_18@__inference___call___1379\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_1524\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Cast' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Cast:\", \"Cast_18@__inference___call___1379\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_1524\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \nTF Select ops: Cast, RealDiv\nDetails:\n\ttf.Cast(tensor<?xf64>) -> (tensor<?xi64>) : {Truncate = false, device = \"\"}\n\ttf.Cast(tensor<?xi64>) -> (tensor<?xf64>) : {Truncate = false, device = \"\"}\n\ttf.RealDiv(tensor<?xf64>, tensor<f64>) -> (tensor<?xf64>) : {device = \"\"}\n\n"
     ]
    }
   ],
   "source": [
    "from onnx_tf.backend import prepare\n",
    "import tensorflow as tf\n",
    "\n",
    "TF_PATH = \"../weights/tflite/yolo_free_large_simplified.pb\" # where the representation of tensorflow model will be stored\n",
    "ONNX_PATH = \"../weights/onnx/11/yolo_free_large_simplified.onnx\" # path to my existing ONNX model\n",
    "onnx_model = onnx.load(ONNX_PATH)  # load onnx model\n",
    "tf_rep = prepare(onnx_model)  # creating TensorflowRep object\n",
    "tf_rep.export_graph(TF_PATH)\n",
    "\n",
    "TFLITE_PATH = \"../weights/tflite/yolo_free_large_simplified.tflite\"\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(TF_PATH)\n",
    "## 启用量化配置\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tf_lite_model = converter.convert()\n",
    "with open(TFLITE_PATH, 'wb') as f:\n",
    "    f.write(tf_lite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
