{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df4678a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Model: YOLOV2 ...\n",
      "==============================\n",
      "Build YOLOV2 ...\n",
      "==============================\n",
      "Model Configuration: \n",
      " {'trans_type': 'ssd', 'multi_scale': [0.5, 1.5], 'backbone': 'darknet19', 'pretrained': True, 'stride': 32, 'neck': 'sppf', 'expand_ratio': 0.5, 'pooling_size': 5, 'neck_act': 'lrelu', 'neck_norm': 'BN', 'neck_depthwise': False, 'head': 'decoupled_head', 'head_act': 'lrelu', 'head_norm': 'BN', 'num_cls_head': 2, 'num_reg_head': 2, 'head_depthwise': False, 'anchor_size': [[17, 25], [55, 75], [92, 206], [202, 21], [289, 311]], 'iou_thresh': 0.5, 'loss_obj_weight': 1.0, 'loss_cls_weight': 1.0, 'loss_box_weight': 5.0, 'no_aug_epoch': -1, 'optimizer': 'sgd', 'momentum': 0.937, 'weight_decay': 0.0005, 'clip_grad': 10, 'ema_decay': 0.9999, 'ema_tau': 2000, 'scheduler': 'linear', 'lr0': 0.01, 'lrf': 0.01, 'warmup_momentum': 0.8, 'warmup_bias_lr': 0.1}\n",
      "==============================\n",
      "Neck: sppf\n",
      "==============================\n",
      "Head: Decoupled Head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (tinynn.converter.operators.torch.prim) fmp_h.1 is of type int64, which is unsupported in TFLite, trying to downcast to int32\n",
      "WARNING (tinynn.converter.operators.torch.prim) fmp_w.1 is of type int64, which is unsupported in TFLite, trying to downcast to int32\n",
      "ERROR (tinynn.converter.base) Unsupported ops: aten::sort\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Cannot continue due to fatal error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 60\u001b[0m\n\u001b[1;32m     56\u001b[0m     converter\u001b[39m.\u001b[39mconvert()\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 60\u001b[0m     main_worker()\n",
      "Cell \u001b[0;32mIn[8], line 56\u001b[0m, in \u001b[0;36mmain_worker\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m# The code section below is used to convert the model to the TFLite format\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m# If you want perform dynamic quantization on the float models,\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m# you may refer to `dynamic.py`, which is in the same folder.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m# As for static quantization (e.g. quantization-aware training and post-training quantization),\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m# please refer to the code examples in the `examples/quantization` folder.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m converter \u001b[39m=\u001b[39m TFLiteConverter(model, dummy_input, output_path)\n\u001b[0;32m---> 56\u001b[0m converter\u001b[39m.\u001b[39;49mconvert()\n",
      "File \u001b[0;32m~/anaconda3/envs/pt20/lib/python3.10/site-packages/TinyNeuralNetwork-0.1.0.20230429124625+2c7dbbde11bf8e7a90377f89a1429639712c0b78-py3.10.egg/tinynn/converter/base.py:485\u001b[0m, in \u001b[0;36mTFLiteConverter.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unsupported_ops) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    484\u001b[0m     log\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnsupported ops: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(unsupported_ops)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 485\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot continue due to fatal error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    487\u001b[0m     optimizer \u001b[39m=\u001b[39m GraphOptimizer(\n\u001b[1;32m    488\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommon_graph,\n\u001b[1;32m    489\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimize,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup_tensors,\n\u001b[1;32m    500\u001b[0m     )\n",
      "\u001b[0;31mException\u001b[0m: Cannot continue due to fatal error"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from tinynn.converter import TFLiteConverter\n",
    "\n",
    "from config import build_model_config\n",
    "from modelx.detectors import build_model\n",
    "\n",
    "class DotDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(DotDict, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def __getattr__(self, key):\n",
    "        value = self[key]\n",
    "        if isinstance(value, dict):\n",
    "            value = DotDict(value)\n",
    "        return value\n",
    "\n",
    "def main_worker():\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    args = {'model':'yolov2', 'num_classes':20, 'conf_thresh':0.1, 'nms_thresh':0.5,\n",
    "            'topk':100,\n",
    "            'weight':'./weights/voc/yolov2/yolov2_epoch_51_75.34.pth'}\n",
    "    args = DotDict(args) \n",
    "\n",
    "    # config\n",
    "    model_cfg = build_model_config(args)\n",
    "\n",
    "    # build model\n",
    "    model = build_model(\n",
    "        args=args, \n",
    "        model_cfg=model_cfg,\n",
    "        device=device, \n",
    "        num_classes=args.num_classes,\n",
    "        trainable=False\n",
    "        )\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    dummy_input = torch.rand((1, 3, 640, 640))\n",
    "\n",
    "    output_path = os.path.join(\"./\", 'weights/onnx/', 'yolov2_epoch_51_75.34.tflite')\n",
    "\n",
    "    # When converting quantized models, please ensure the quantization backend is set.\n",
    "    torch.backends.quantized.engine = 'qnnpack'\n",
    "\n",
    "    # The code section below is used to convert the model to the TFLite format\n",
    "    # If you want perform dynamic quantization on the float models,\n",
    "    # you may refer to `dynamic.py`, which is in the same folder.\n",
    "    # As for static quantization (e.g. quantization-aware training and post-training quantization),\n",
    "    # please refer to the code examples in the `examples/quantization` folder.\n",
    "    converter = TFLiteConverter(model, dummy_input, output_path)\n",
    "    converter.convert()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main_worker()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
